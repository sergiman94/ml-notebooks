{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP CONSTRUCTING TEXT GENERATION MODEL"
      ],
      "metadata": {
        "id": "B1go8hDTE32B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da1b1644-921d-4729-8614-8c02dda549bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-31 14:45:26--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 142.251.6.101, 142.251.6.100, 142.251.6.102, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.251.6.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/uh77t2f5ipauciv0nd9fgd091ej88hdh/1654008300000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2022-05-31 14:45:28--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/uh77t2f5ipauciv0nd9fgd091ej88hdh/1654008300000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 142.251.120.132, 2607:f8b0:4001:c2e::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|142.251.120.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   188MB/s    in 0.4s    \n",
            "\n",
            "2022-05-31 14:45:29 (188 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fdf15a2-9e91-419a-efac-784897da78ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length\n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167a5fec-a141-409e-90ea-ce6be008a7b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8323b9a-5585-4afd-aabd-5b3c2c3392da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 8s 7ms/step - loss: 5.9698 - accuracy: 0.0363\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.4324 - accuracy: 0.0399\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3628 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3047 - accuracy: 0.0373\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.2297 - accuracy: 0.0399\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.1519 - accuracy: 0.0525\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0721 - accuracy: 0.0570\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0050 - accuracy: 0.0555\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.9264 - accuracy: 0.0626\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8398 - accuracy: 0.0777\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.7647 - accuracy: 0.0762\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.6814 - accuracy: 0.0787\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.6007 - accuracy: 0.0843\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5292 - accuracy: 0.0943\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4536 - accuracy: 0.0949\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3744 - accuracy: 0.1014\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2920 - accuracy: 0.1145\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2237 - accuracy: 0.1271\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1457 - accuracy: 0.1382\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0695 - accuracy: 0.1761\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9945 - accuracy: 0.1751\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9175 - accuracy: 0.1887\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8583 - accuracy: 0.2094\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7959 - accuracy: 0.2200\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7304 - accuracy: 0.2306\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6600 - accuracy: 0.2351\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5902 - accuracy: 0.2639\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5288 - accuracy: 0.2760\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4614 - accuracy: 0.2830\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3985 - accuracy: 0.2891\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3347 - accuracy: 0.3052\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2728 - accuracy: 0.3128\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2130 - accuracy: 0.3350\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1633 - accuracy: 0.3355\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1174 - accuracy: 0.3542\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0543 - accuracy: 0.3673\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0049 - accuracy: 0.3759\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9482 - accuracy: 0.3895\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8871 - accuracy: 0.4122\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8478 - accuracy: 0.4097\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.7983 - accuracy: 0.4248\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7438 - accuracy: 0.4314\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.7107 - accuracy: 0.4490\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6627 - accuracy: 0.4521\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.6134 - accuracy: 0.4687\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6250 - accuracy: 0.4521\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5433 - accuracy: 0.4682\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5132 - accuracy: 0.4682\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4753 - accuracy: 0.4844\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4279 - accuracy: 0.4990\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3992 - accuracy: 0.5045\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3461 - accuracy: 0.5131\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2999 - accuracy: 0.5217\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2500 - accuracy: 0.5399\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2209 - accuracy: 0.5414\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1988 - accuracy: 0.5459\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1422 - accuracy: 0.5626\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1027 - accuracy: 0.5691\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0876 - accuracy: 0.5661\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0870 - accuracy: 0.5616\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0247 - accuracy: 0.5706\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9831 - accuracy: 0.5873\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9715 - accuracy: 0.5832\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0069 - accuracy: 0.5691\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9133 - accuracy: 0.5908\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8697 - accuracy: 0.6009\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8284 - accuracy: 0.6110\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7978 - accuracy: 0.6130\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7663 - accuracy: 0.6307\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7347 - accuracy: 0.6352\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7350 - accuracy: 0.6347\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7191 - accuracy: 0.6367\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6799 - accuracy: 0.6498\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6380 - accuracy: 0.6514\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6262 - accuracy: 0.6493\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5988 - accuracy: 0.6599\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5597 - accuracy: 0.6715\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5523 - accuracy: 0.6700\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5652 - accuracy: 0.6625\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5407 - accuracy: 0.6720\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5019 - accuracy: 0.6811\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4551 - accuracy: 0.6927\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4268 - accuracy: 0.6968\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4173 - accuracy: 0.6963\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4192 - accuracy: 0.6963\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4082 - accuracy: 0.7033\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4441 - accuracy: 0.6786\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4030 - accuracy: 0.6983\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3386 - accuracy: 0.7149\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3067 - accuracy: 0.7210\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2798 - accuracy: 0.7281\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2637 - accuracy: 0.7326\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2408 - accuracy: 0.7361\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2215 - accuracy: 0.7321\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2118 - accuracy: 0.7437\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1968 - accuracy: 0.7392\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1726 - accuracy: 0.7492\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1543 - accuracy: 0.7497\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1380 - accuracy: 0.7528\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1246 - accuracy: 0.7634\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1193 - accuracy: 0.7573\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0986 - accuracy: 0.7603\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0796 - accuracy: 0.7699\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0701 - accuracy: 0.7679\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0734 - accuracy: 0.7750\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0573 - accuracy: 0.7689\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0738 - accuracy: 0.7674\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0639 - accuracy: 0.7674\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0312 - accuracy: 0.7810\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 1.0102 - accuracy: 0.7856\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9924 - accuracy: 0.7866\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 0.9781 - accuracy: 0.7931\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 1.0022 - accuracy: 0.7795\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.1225 - accuracy: 0.7528\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0227 - accuracy: 0.7760\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9986 - accuracy: 0.7815\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9624 - accuracy: 0.7926\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9411 - accuracy: 0.7987\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9350 - accuracy: 0.7931\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9070 - accuracy: 0.8063\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8917 - accuracy: 0.8047\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8826 - accuracy: 0.8037\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8739 - accuracy: 0.8083\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8639 - accuracy: 0.8058\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8558 - accuracy: 0.8158\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8403 - accuracy: 0.8143\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.8269 - accuracy: 0.8189\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8143 - accuracy: 0.8219\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8073 - accuracy: 0.8194\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8055 - accuracy: 0.8184\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7924 - accuracy: 0.8219\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7818 - accuracy: 0.8295\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7720 - accuracy: 0.8254\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7658 - accuracy: 0.8264\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7808 - accuracy: 0.8254\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7599 - accuracy: 0.8320\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7689 - accuracy: 0.8194\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7660 - accuracy: 0.8254\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7518 - accuracy: 0.8305\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7362 - accuracy: 0.8335\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7267 - accuracy: 0.8340\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7115 - accuracy: 0.8325\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7461 - accuracy: 0.8264\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7185 - accuracy: 0.8269\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6897 - accuracy: 0.8406\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6857 - accuracy: 0.8401\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6729 - accuracy: 0.8411\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6598 - accuracy: 0.8451\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6657 - accuracy: 0.8481\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6565 - accuracy: 0.8456\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6421 - accuracy: 0.8496\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6348 - accuracy: 0.8522\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6303 - accuracy: 0.8562\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6257 - accuracy: 0.8577\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6186 - accuracy: 0.8572\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6146 - accuracy: 0.8557\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6126 - accuracy: 0.8572\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6089 - accuracy: 0.8537\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5988 - accuracy: 0.8602\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5890 - accuracy: 0.8623\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5829 - accuracy: 0.8623\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5852 - accuracy: 0.8567\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5905 - accuracy: 0.8602\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5906 - accuracy: 0.8643\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5733 - accuracy: 0.8658\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5688 - accuracy: 0.8693\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6192 - accuracy: 0.8502\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6415 - accuracy: 0.8406\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6206 - accuracy: 0.8512\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6156 - accuracy: 0.8527\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5790 - accuracy: 0.8628\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5653 - accuracy: 0.8648\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5521 - accuracy: 0.8638\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5405 - accuracy: 0.8729\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5310 - accuracy: 0.8693\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5689 - accuracy: 0.8638\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5645 - accuracy: 0.8602\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5372 - accuracy: 0.8628\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5324 - accuracy: 0.8713\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5311 - accuracy: 0.8613\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5139 - accuracy: 0.8744\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5119 - accuracy: 0.8734\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5126 - accuracy: 0.8703\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5055 - accuracy: 0.8708\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4980 - accuracy: 0.8703\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4968 - accuracy: 0.8729\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5534 - accuracy: 0.8542\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5827 - accuracy: 0.8416\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5118 - accuracy: 0.8643\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5375 - accuracy: 0.8577\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5612 - accuracy: 0.8491\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5078 - accuracy: 0.8713\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4864 - accuracy: 0.8693\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4750 - accuracy: 0.8734\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4682 - accuracy: 0.8759\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4882 - accuracy: 0.8729\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5915 - accuracy: 0.8411\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5454 - accuracy: 0.8567\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5029 - accuracy: 0.8628\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4905 - accuracy: 0.8693\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "39bd9625-104a-40f1-d62e-c95cc8515980"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96294228-71ca-44ce-d995-79931a802591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling chills me and of warning to wait talk found you broken care used found i had found crazy had eyes power darkest found crazy world feeling crazy park park darkest life found god could had found crazy world eyes dumb feet break shadow park boomaboomerang found found i had found crazy world feeling crazy park park dumb park park park park eyes night night park eyes shame night park night park park eyes shadow night night eyes shame break breeze shadow park park crazy joe andante break park park crazy joe joe park life god could had found crazy had found\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}